name: Benchmark Execution

on:
  workflow_dispatch:
    inputs:
      duration:
        description: 'Duration in minutes per test (single-QPS mode)'
        required: true
        default: '15'
      qps:
        description: 'Target QPS (single-QPS mode)'
        required: true
        default: '500'
      tier:
        description: 'Infrastructure tier (small/medium/large)'
        required: false
        default: 'small'
      sweep:
        description: 'Enable QPS sweep mode'
        required: false
        type: boolean
        default: false
      qps_levels:
        description: 'Comma-separated QPS levels for sweep mode'
        required: false
        default: '50,100,250,500'

permissions:
  contents: write # To update README
  pull-requests: write # To create PRs
  id-token: write

env:
  PROJECT_ID: ${{ vars.GCP_PROJECT_ID }}
  WIF_PROVIDER: ${{ vars.GCP_WIF_PROVIDER }}
  WIF_SERVICE_ACCOUNT: ${{ vars.GCP_WIF_SA }}
  REGION: us-central1
  ZONE: us-central1-a
  CLUSTER_NAME: ctlog-cluster

jobs:
  benchmark:
    name: Run Benchmark
    runs-on: ubuntu-latest
    environment: BENCHMARKING

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v3
      with:
        workload_identity_provider: ${{ env.WIF_PROVIDER }}
        service_account: ${{ env.WIF_SERVICE_ACCOUNT }}

    - name: Setup GCloud & Kubectl
      uses: google-github-actions/setup-gcloud@v2
      with:
        project_id: ${{ env.PROJECT_ID }}
        install_components: 'gke-gcloud-auth-plugin'

    - name: Get Cluster Credentials
      uses: google-github-actions/get-gke-credentials@v2
      with:
        cluster_name: ${{ env.CLUSTER_NAME }}
        location: ${{ env.ZONE }}

    - name: Setup Go
      uses: actions/setup-go@v5
      with:
        go-version: '1.24'

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install Python Deps
      run: pip install -r scripts/requirements.txt

    - name: Run Benchmark
      id: bench
      run: |
        if [ "${{ inputs.sweep }}" = "true" ]; then
          python3 scripts/benchmark.py \
            --project_id ${{ env.PROJECT_ID }} \
            --tier ${{ inputs.tier }} \
            --qps_levels ${{ inputs.qps_levels }} \
            --sweep_duration ${{ inputs.duration }} \
            --warmup 60 2>&1 | tee benchmark_output.txt
        else
          python3 scripts/benchmark.py \
            --project_id ${{ env.PROJECT_ID }} \
            --tier ${{ inputs.tier }} \
            --duration ${{ inputs.duration }} \
            --qps ${{ inputs.qps }} \
            --warmup 60 2>&1 | tee benchmark_output.txt
        fi

    - name: Update README
      if: success()
      run: |
        python3 scripts/update_readme.py

    - name: Generate Report
      if: success()
      run: |
        python3 scripts/report.py benchmark_summary.json > REPORT.md
        cat REPORT.md >> $GITHUB_STEP_SUMMARY

    - name: Parse Results
      if: success()
      run: |
        # Extract best values from benchmark_summary.json for the PR body
        TR_QPS=$(python3 -c "
import json
d=json.load(open('benchmark_summary.json'))
results = d['results'] if isinstance(d, dict) else d
tr = [r for r in results if r['log_type']=='trillian']
print(f\"{max(r['achieved_qps'] for r in tr):.2f}\" if tr else 'N/A')
")
        TE_QPS=$(python3 -c "
import json
d=json.load(open('benchmark_summary.json'))
results = d['results'] if isinstance(d, dict) else d
te = [r for r in results if r['log_type']=='tesseract']
print(f\"{max(r['achieved_qps'] for r in te):.2f}\" if te else 'N/A')
")
        TR_COST_HR=$(python3 -c "
import json
d=json.load(open('benchmark_summary.json'))
results = d['results'] if isinstance(d, dict) else d
tr = [r for r in results if r['log_type']=='trillian']
best = max(tr, key=lambda r: r['achieved_qps'])
print(f\"{best['cost_per_hour']:.4f}\")
")
        TE_COST_HR=$(python3 -c "
import json
d=json.load(open('benchmark_summary.json'))
results = d['results'] if isinstance(d, dict) else d
te = [r for r in results if r['log_type']=='tesseract']
best = max(te, key=lambda r: r['achieved_qps'])
print(f\"{best['cost_per_hour']:.4f}\")
")
        TR_COST_1M=$(python3 -c "
import json
d=json.load(open('benchmark_summary.json'))
results = d['results'] if isinstance(d, dict) else d
tr = [r for r in results if r['log_type']=='trillian']
best = max(tr, key=lambda r: r['achieved_qps'])
print(f\"{best['cost_per_1m_entries']:.2f}\" if best.get('cost_per_1m_entries',0)>0 else 'N/A')
")
        TE_COST_1M=$(python3 -c "
import json
d=json.load(open('benchmark_summary.json'))
results = d['results'] if isinstance(d, dict) else d
te = [r for r in results if r['log_type']=='tesseract']
best = max(te, key=lambda r: r['achieved_qps'])
print(f\"{best['cost_per_1m_entries']:.2f}\" if best.get('cost_per_1m_entries',0)>0 else 'N/A')
")

        echo "TR_QPS=$TR_QPS" >> $GITHUB_ENV
        echo "TE_QPS=$TE_QPS" >> $GITHUB_ENV
        echo "TR_COST_HR=$TR_COST_HR" >> $GITHUB_ENV
        echo "TE_COST_HR=$TE_COST_HR" >> $GITHUB_ENV
        echo "TR_COST_1M=$TR_COST_1M" >> $GITHUB_ENV
        echo "TE_COST_1M=$TE_COST_1M" >> $GITHUB_ENV

    - name: Create Pull Request
      if: success()
      uses: peter-evans/create-pull-request@v6
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        commit-message: "Docs: Update benchmark results"
        title: "Benchmark Results: ${{ inputs.tier }} tier @ ${{ inputs.sweep == 'true' && inputs.qps_levels || inputs.qps }} QPS"
        body: |
          ## Results â€” Tier: ${{ inputs.tier }}

          | Metric | Trillian (MySQL) | TesseraCT (Spanner) |
          |---|---|---|
          | **Peak QPS** | ${{ env.TR_QPS }} | ${{ env.TE_QPS }} |
          | **Infra Cost/hr** | $${{ env.TR_COST_HR }} | $${{ env.TE_COST_HR }} |
          | **Cost per 1M Entries** | $${{ env.TR_COST_1M }} | $${{ env.TE_COST_1M }} |

          See full report in `REPORT.md` and raw data in `benchmark_summary.json`.

          Mode: ${{ inputs.sweep == 'true' && 'QPS sweep' || 'Single QPS' }}, Tier: ${{ inputs.tier }}
        branch: "benchmark-results-${{ github.run_id }}"
        base: main
        add-paths: |
          REPORT.md
          README.md
          benchmark_summary.json
