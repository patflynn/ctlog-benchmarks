name: Benchmark Execution

on:
  workflow_dispatch:
    inputs:
      duration:
        description: 'Duration in minutes per test (single-QPS mode)'
        required: true
        default: '5'
      qps:
        description: 'Target QPS (single-QPS mode)'
        required: true
        default: '50'
      tier:
        description: 'Infrastructure tier (small/medium/large)'
        required: false
        default: 'large'
      sweep:
        description: 'Enable QPS sweep mode'
        required: false
        type: boolean
        default: false
      qps_levels:
        description: 'Comma-separated QPS levels for sweep mode'
        required: false
        default: 'auto'

concurrency:
  group: benchmark
  cancel-in-progress: true

permissions:
  contents: write # To update README
  pull-requests: write # To create PRs
  id-token: write

env:
  PROJECT_ID: ${{ vars.GCP_PROJECT_ID }}
  WIF_PROVIDER: ${{ vars.GCP_WIF_PROVIDER }}
  WIF_SERVICE_ACCOUNT: ${{ vars.GCP_WIF_SA }}
  REGION: us-central1
  ZONE: us-central1-a
  CLUSTER_NAME: ctlog-cluster

jobs:
  benchmark:
    name: Run Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 75
    environment: BENCHMARKING

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v3
      with:
        workload_identity_provider: ${{ env.WIF_PROVIDER }}
        service_account: ${{ env.WIF_SERVICE_ACCOUNT }}

    - name: Setup GCloud & Kubectl
      uses: google-github-actions/setup-gcloud@v2
      with:
        project_id: ${{ env.PROJECT_ID }}
        install_components: 'gke-gcloud-auth-plugin'

    - name: Get Cluster Credentials
      uses: google-github-actions/get-gke-credentials@v2
      with:
        cluster_name: ${{ env.CLUSTER_NAME }}
        location: ${{ env.ZONE }}

    - name: Setup Go
      uses: actions/setup-go@v5
      with:
        go-version: '1.24'

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install Python Deps
      run: pip install -r scripts/requirements.txt

    - name: Run Benchmark
      id: bench
      run: |
        set -o pipefail
        if [ "${{ inputs.sweep }}" = "true" ]; then
          python3 scripts/benchmark.py \
            --project_id ${{ env.PROJECT_ID }} \
            --tier ${{ inputs.tier }} \
            --qps_levels ${{ inputs.qps_levels }} \
            --sweep_duration ${{ inputs.duration }} \
            --warmup 60 2>&1 | tee benchmark_output.txt
        else
          python3 scripts/benchmark.py \
            --project_id ${{ env.PROJECT_ID }} \
            --tier ${{ inputs.tier }} \
            --duration ${{ inputs.duration }} \
            --qps ${{ inputs.qps }} \
            --warmup 60 2>&1 | tee benchmark_output.txt
        fi

    - name: Update README
      if: success()
      run: |
        python3 scripts/update_readme.py

    - name: Generate Report
      if: success()
      run: |
        python3 scripts/report.py benchmark_summary.json > REPORT.md
        cat REPORT.md >> $GITHUB_STEP_SUMMARY

    - name: Parse Results
      if: success()
      run: |
        python3 - <<'PYEOF'
        import json, os
        d = json.load(open('benchmark_summary.json'))
        results = d['results'] if isinstance(d, dict) else d
        def best(log_type):
            hits = [r for r in results if r['log_type'] == log_type]
            return max(hits, key=lambda r: r['achieved_qps']) if hits else None
        tr = best('trillian')
        te = best('tesseract')
        env = os.environ.get('GITHUB_ENV', '/dev/null')
        with open(env, 'a') as f:
            f.write(f"TR_QPS={tr['achieved_qps']:.2f}\n" if tr else 'TR_QPS=N/A\n')
            f.write(f"TE_QPS={te['achieved_qps']:.2f}\n" if te else 'TE_QPS=N/A\n')
            f.write(f"TR_COST_HR={tr['cost_per_hour']:.4f}\n" if tr else 'TR_COST_HR=N/A\n')
            f.write(f"TE_COST_HR={te['cost_per_hour']:.4f}\n" if te else 'TE_COST_HR=N/A\n')
            f.write(f"TR_COST_1M={tr['cost_per_1m_entries']:.2f}\n" if tr and tr.get('cost_per_1m_entries',0)>0 else 'TR_COST_1M=N/A\n')
            f.write(f"TE_COST_1M={te['cost_per_1m_entries']:.2f}\n" if te and te.get('cost_per_1m_entries',0)>0 else 'TE_COST_1M=N/A\n')
        PYEOF

    - name: Create Pull Request
      if: success()
      uses: peter-evans/create-pull-request@v6
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        commit-message: "Docs: Update benchmark results"
        title: "Benchmark Results: ${{ inputs.tier }} tier @ ${{ inputs.sweep == 'true' && inputs.qps_levels || inputs.qps }} QPS"
        body: |
          ## Results â€” Tier: ${{ inputs.tier }}

          | Metric | Trillian (MySQL) | TesseraCT (Spanner) |
          |---|---|---|
          | **Peak QPS** | ${{ env.TR_QPS }} | ${{ env.TE_QPS }} |
          | **Infra Cost/hr** | $${{ env.TR_COST_HR }} | $${{ env.TE_COST_HR }} |
          | **Cost per 1M Entries** | $${{ env.TR_COST_1M }} | $${{ env.TE_COST_1M }} |

          See full report in `REPORT.md` and raw data in `benchmark_summary.json`.

          Mode: ${{ inputs.sweep == 'true' && 'QPS sweep' || 'Single QPS' }}, Tier: ${{ inputs.tier }}
        branch: "benchmark-results-${{ github.run_id }}"
        base: main
        add-paths: |
          REPORT.md
          README.md
          benchmark_summary.json
