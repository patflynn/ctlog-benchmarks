name: Benchmark Execution

on:
  workflow_dispatch:
    inputs:
      duration:
        description: 'Duration in minutes per test'
        required: true
        default: '15'
      qps:
        description: 'Target QPS'
        required: true
        default: '500'

permissions:
  contents: write # To update README
  pull-requests: write # To create PRs
  id-token: write

env:
  PROJECT_ID: ${{ vars.GCP_PROJECT_ID }}
  WIF_PROVIDER: ${{ vars.GCP_WIF_PROVIDER }}
  WIF_SERVICE_ACCOUNT: ${{ vars.GCP_WIF_SA }}
  REGION: us-central1
  ZONE: us-central1-a
  CLUSTER_NAME: ctlog-cluster

jobs:
  benchmark:
    name: Run Benchmark
    runs-on: ubuntu-latest
    environment: BENCHMARKING

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v3
      with:
        workload_identity_provider: ${{ env.WIF_PROVIDER }}
        service_account: ${{ env.WIF_SERVICE_ACCOUNT }}

    - name: Setup GCloud & Kubectl
      uses: google-github-actions/setup-gcloud@v2
      with:
        project_id: ${{ env.PROJECT_ID }}
        install_components: 'gke-gcloud-auth-plugin'

    - name: Get Cluster Credentials
      uses: google-github-actions/get-gke-credentials@v2
      with:
        cluster_name: ${{ env.CLUSTER_NAME }}
        location: ${{ env.ZONE }}

    - name: Setup Go
      uses: actions/setup-go@v5
      with:
        go-version: '1.24'

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install Python Deps
      run: pip install -r scripts/requirements.txt

    - name: Run Benchmark Script
      id: bench
      run: |
        python3 scripts/benchmark.py \
          --project_id ${{ env.PROJECT_ID }} \
          --duration ${{ inputs.duration }} \
          --qps ${{ inputs.qps }} 2>&1 | tee benchmark_output.txt

    - name: Update README
      if: success()
      run: |
        python3 scripts/update_readme.py

    - name: Parse Results
      if: success()
      run: |
        # Extract values from benchmark_summary.json for the PR body
        TR_QPS=$(python3 -c "import json; d=json.load(open('benchmark_summary.json')); r=next(x for x in d if x['log_type']=='trillian'); print(f\"{r['achieved_qps']:.2f}\")")
        TE_QPS=$(python3 -c "import json; d=json.load(open('benchmark_summary.json')); r=next(x for x in d if x['log_type']=='tesseract'); print(f\"{r['achieved_qps']:.2f}\")")
        TR_COST_HR=$(python3 -c "import json; d=json.load(open('benchmark_summary.json')); r=next(x for x in d if x['log_type']=='trillian'); print(f\"{r['cost_per_hour']:.4f}\")")
        TE_COST_HR=$(python3 -c "import json; d=json.load(open('benchmark_summary.json')); r=next(x for x in d if x['log_type']=='tesseract'); print(f\"{r['cost_per_hour']:.4f}\")")
        TR_TOTAL=$(python3 -c "import json; d=json.load(open('benchmark_summary.json')); r=next(x for x in d if x['log_type']=='trillian'); print(f\"{r['total_cost']:.4f}\")")
        TE_TOTAL=$(python3 -c "import json; d=json.load(open('benchmark_summary.json')); r=next(x for x in d if x['log_type']=='tesseract'); print(f\"{r['total_cost']:.4f}\")")
        TR_COST_1M=$(python3 -c "import json; d=json.load(open('benchmark_summary.json')); r=next(x for x in d if x['log_type']=='trillian'); qps=r['achieved_qps']; print(f\"{r['cost_per_hour']/(qps*3600)*1e6:.2f}\" if qps>0 else 'N/A')")
        TE_COST_1M=$(python3 -c "import json; d=json.load(open('benchmark_summary.json')); r=next(x for x in d if x['log_type']=='tesseract'); qps=r['achieved_qps']; print(f\"{r['cost_per_hour']/(qps*3600)*1e6:.2f}\" if qps>0 else 'N/A')")

        echo "TR_QPS=$TR_QPS" >> $GITHUB_ENV
        echo "TE_QPS=$TE_QPS" >> $GITHUB_ENV
        echo "TR_COST_HR=$TR_COST_HR" >> $GITHUB_ENV
        echo "TE_COST_HR=$TE_COST_HR" >> $GITHUB_ENV
        echo "TR_TOTAL=$TR_TOTAL" >> $GITHUB_ENV
        echo "TE_TOTAL=$TE_TOTAL" >> $GITHUB_ENV
        echo "TR_COST_1M=$TR_COST_1M" >> $GITHUB_ENV
        echo "TE_COST_1M=$TE_COST_1M" >> $GITHUB_ENV

    - name: Publish Results
      if: success()
      continue-on-error: true
      run: |
        DATE=$(date +%Y-%m-%d)

        cat > RESULTS.md << HEREDOC
        ## Benchmark Results ($DATE)

        | Metric | Trillian (MySQL) | TesseraCT (Spanner) |
        |---|---|---|
        | **Duration** | ${{ inputs.duration }}m | ${{ inputs.duration }}m |
        | **Target QPS** | ${{ inputs.qps }} | ${{ inputs.qps }} |
        | **Achieved QPS** | $TR_QPS | $TE_QPS |
        | **Infra Cost/hr** | \$$TR_COST_HR | \$$TE_COST_HR |
        | **Benchmark Cost** | \$$TR_TOTAL | \$$TE_TOTAL |
        | **Cost per 1M Entries** | \$$TR_COST_1M | \$$TE_COST_1M |

        Cost model: deterministic from [costs.json](costs.json) (Terraform-derived infrastructure rates).
        HEREDOC

        cat RESULTS.md >> $GITHUB_STEP_SUMMARY

    - name: Create Pull Request
      if: success()
      uses: peter-evans/create-pull-request@v6
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        commit-message: "Docs: Update benchmark results"
        title: "Benchmark Results: ${{ inputs.duration }}m @ ${{ inputs.qps }} QPS"
        body: |
          ## Results

          | Metric | Trillian (MySQL) | TesseraCT (Spanner) |
          |---|---|---|
          | **Achieved QPS** | ${{ env.TR_QPS }} | ${{ env.TE_QPS }} |
          | **Infra Cost/hr** | $${{ env.TR_COST_HR }} | $${{ env.TE_COST_HR }} |
          | **Cost per 1M Entries** | $${{ env.TR_COST_1M }} | $${{ env.TE_COST_1M }} |

          ## Cost Model

          Fixed infrastructure costs from [`costs.json`](costs.json):
          - **Trillian:** $0.178/hr (shared GKE) + $0.015/hr (Cloud SQL db-f1-micro) = **$0.193/hr**
          - **TesseraCT:** $0.178/hr (shared GKE) + $0.090/hr (Spanner 100 PU) = **$0.268/hr**
          - **Cost per 1M** = (cost_per_hour / achieved_qps / 3600) Ã— 1,000,000

          Duration: ${{ inputs.duration }}m, Target QPS: ${{ inputs.qps }}
        branch: "benchmark-results-${{ github.run_id }}"
        base: main
        add-paths: |
          RESULTS.md
          README.md
          benchmark_summary.json
